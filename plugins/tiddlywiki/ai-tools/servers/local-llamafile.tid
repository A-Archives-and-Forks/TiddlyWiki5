title: $:/plugins/tiddlywiki/ai-tools/servers/local-llamafile
tags: $:/tags/AI/CompletionServer
url: http://127.0.0.1:8080/v1/chat/completions
caption: Locally running Llamafile server

<!--
Wikified JSON text to be sent to server
-->
\procedure json-prompt()
\rules only filteredtranscludeinline transcludeinline macrodef macrocallinline html conditional commentblock commentinline
{
	"model": "gpt-4o",
	"messages": [
		{
			"role": "system",
			"content": "<$text text={{{ [<conversationTitle>get[system-prompt]jsonstringify[]] }}}/>"
		}
		<!-- Loop through the tiddlers tagged with this one to pick up all the messages in the conversation -->
		<$list filter="[all[shadows+tiddlers]tag<conversationTitle>!is[draft]sort[created]]">
			,
			{
				<!-- We use JSON stringify to escape the characters that can't be used directly in JSON -->
				"role": "<$text text={{{ [<currentTiddler>get[role]jsonstringify[]] }}}/>",
				"content": "<$text text={{{ [<currentTiddler>get[text]jsonstringify[]] }}}/>"
			}
		</$list>
	]
}
\end json-prompt

<!--
Callback for the HTTP response from the LLM
-->
\procedure completion-callback()
	<%if [<status>compare:number:gteq[200]compare:number:lteq[299]] %>
		<!-- Success -->
		<$action-createtiddler
			$basetitle=<<resultTitlePrefix>>
			tags=<<resultTags>>
			type="text/markdown"
			role={{{ [<data>jsonget[choices],[0],[message],[role]] }}}
			text={{{ [<data>jsonget[choices],[0],[message],[content]] }}}
		/>
	<%else%>
		<!-- Error -->
		<$action-createtiddler
			$basetitle=<<resultTitlePrefix>>
			tags=<<resultTags>>
			type="text/markdown"
			role="error"
			text={{{ [[Error:]] [<statusText>] [<data>jsonget[error],[message]]  +[join[]] }}}
		/>
	<%endif%>
\end completion-callback
